{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wally287/hello--streamlit/blob/main/Class_1_LatentSpace_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  This Python code is actually a demo of building, training a basic autoencoder neural network on MNIST Dataset using PyTorch. It starts with importing all required tools and libraries such as: torch, torchvision, and matplotlib. Torch does the heavy lifting for the neural net- model definition- torchvision for the dataset loading, and matplotlib is for visualization.\n",
        "\n",
        "# The Autoencoder class architecture is basically two parts: the encoder shrinks those 28x28 pixel images down into a lower dimension latent vector, and the decoder tries to reconstruct them back up to the original size. Both sides are implemented as fully connected layers, with ReLU and Sigmoid for nonlinear activation. The main point is that the model will figure out the best way to flatten and unflatten the images without totally changing them (to capture meaningful features of the data while minimizing the difference between the input and its reconstruction).\n",
        "\n",
        "# MNIST gets downloaded, turned into tensors, and normalized so the pixel values range between -1 and 1. DataLoader is utilized to cut everything into batches, making training more efficient. Also, the code is smart enough to use GPU -if it is available- otherwise, it uses the CPU.\n",
        "\n",
        "# For training, it goes with a loss function - MSELoss (compares pixel values of the original and reconstructed images) and applies optimization to update weights. Also, it uses mixed precision (GradScaler and autocast) to make training faster on GPUs and less memory usage (for efficiency without sacrificing accuracy).\n",
        "\n",
        "# Finally, Training starts, for the specified number of epochs (10) ,  each epoch runs images through the autoencoder, measures how good or bad the reconstructions are, computes the loss, and tweaks the weights. After training, the model is switched to evaluation mode, and both the original and reconstructed images are visualized side by side using imshow and torchvision.utils.make_grid.\n",
        "\n",
        "# Once done, it flips to evaluation mode, and we get a side-by-side comparison of the originals and the reconstructions—MNIST edition. It also shows us the compressed latent vectors for a handful of digits, so we can peek inside what the encoder’s doing and how the network compresses data.\n",
        "# Overall, this code demonstrates how to implement, train, and analyze a simple autoencoder using PyTorch with modern GPU acceleration techniques-if available."
      ],
      "metadata": {
        "id": "1w1oKT3_f0Qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code defines and utilizes an Autoencoder neural network for the MNIST dataset. Here's a detailed explanation:\n",
        "\n",
        "- **Autoencoder Class**: This class creates a neural network with encoder and decoder components. The encoder compresses the input image into a lower-dimensional latent space, and the decoder reconstructs the image from this latent space. The network is structured to flatten and process 28x28 grayscale MNIST images.\n",
        "\n",
        "- **Data Loading**: Utilizes PyTorch's DataLoader to efficiently load the MNIST dataset, applying transformations to normalize the images.\n",
        "\n",
        "- **Training Loop**: Iterates over the training dataset, feeding batches of images through the model, calculating the reconstruction loss, and updating the model's weights to minimize this loss, effectively learning to compress and reconstruct the input images.\n",
        "\n",
        "- **Visualization**: After training, the script visualizes a batch of original images and their reconstructions from the autoencoder. It also prints the latent space representations, showcasing what the model has learned to encode.\n",
        "\n",
        "- **Utility Functions**: Includes `imshow` for displaying tensors as images. It unnormalizes the data and uses Matplotlib to plot them.\n",
        "\n",
        "This script encapsulates the end-to-end process of training an autoencoder on the MNIST dataset, visualizing the results, and examining the learned latent space."
      ],
      "metadata": {
        "id": "MXble1sY6GuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "from torch.cuda.amp import GradScaler, autocast"
      ],
      "metadata": {
        "id": "QTp8sYxwBg-1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " The `Autoencoder` class inherits from `nn.Module`, a base class for all neural network modules in PyTorch. Here's a breakdown of the code and its functionality:\n",
        "\n",
        "### Initialization Method (`__init__`)\n",
        "- **Parameters**: The `__init__` method accepts a single parameter `encoding_dim`, which specifies the size of the latent space where the input data is compressed.\n",
        "- **Encoder**: The encoder part of the autoencoder is designed to compress the input data (in this case, an image) into a lower-dimensional representation called the latent space. It consists of a sequence of layers:\n",
        "  - `nn.Linear(28 * 28, 128)`: This layer flattens the input image (assumed to be 28x28 pixels, typical for MNIST dataset images) into a one-dimensional array and performs a linear transformation to reduce its dimension to 128.\n",
        "  - `nn.ReLU(True)`: A Rectified Linear Unit (ReLU) activation function is applied to introduce non-linearity, helping the model learn complex patterns.\n",
        "  - Another `nn.Linear(128, encoding_dim)`: Further reduces the dimension from 128 to the specified `encoding_dim`.\n",
        "  - Another `nn.ReLU(True)`: Another ReLU activation for non-linearity.\n",
        "- **Decoder**: The decoder part reconstructs the original input data from the compressed representation. It mirrors the encoder structure but in reverse, aiming to expand the compressed data back to its original shape:\n",
        "  - `nn.Linear(encoding_dim, 128)`: Expands the compressed data from `encoding_dim` back to 128.\n",
        "  - `nn.ReLU(True)`: Applies ReLU activation.\n",
        "  - `nn.Linear(128, 28 * 28)`: Transforms the data from 128 back to the flattened image size of 784 (28x28).\n",
        "  - `nn.Sigmoid()`: Applies a sigmoid activation function to ensure the output values are between 0 and 1, suitable for image data where pixel values typically fall within this range.\n",
        "\n",
        "### Forward Method (`forward`)\n",
        "- **Parameter**: The `forward` method defines how the input `x` flows through the network.\n",
        "- **Process**:\n",
        "  - `x.view(-1, 28*28)`: First, the input `x` is reshaped into a one-dimensional array (flattened) if not already done.\n",
        "  - `self.encoder(x)`: The flattened `x` is then passed through the encoder.\n",
        "  - `self.decoder(x)`: The output from the encoder, which is the compressed representation, is fed into the decoder.\n",
        "- **Output**: The final output is reshaped back to the original image dimensions (`-1, 1, 28, 28`), where `-1` is a placeholder that automatically adjusts based on the batch size.\n"
      ],
      "metadata": {
        "id": "Lth1Gula4Sz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder: compresses the image into a lower-dimensional latent space\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 128),  # Flatten the image and then linearly transform it\n",
        "            nn.ReLU(True),  # Non-linear activation function\n",
        "            nn.Linear(128, encoding_dim),  # Linear transformation to the encoding dimension\n",
        "            nn.ReLU(True)  # Non-linear activation function\n",
        "        )\n",
        "        # Decoder: reconstructs the image from the latent space\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 128),  # Linearly transforms the encoding\n",
        "            nn.ReLU(True),  # Non-linear activation function\n",
        "            nn.Linear(128, 28 * 28),  # Transforms back to original image shape\n",
        "            nn.Sigmoid()  # Sigmoid activation to output values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x.view(-1, 28*28))  # Encode the input image\n",
        "        x = self.decoder(x)  # Decode the encoded image\n",
        "        return x.view(-1, 1, 28, 28)  # Reshape to the original image dimensions"
      ],
      "metadata": {
        "id": "wWlX07fEBjL8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def imshow(img):\n",
        "    img = img.cpu() / 2 + 0.5  # Unnormalize the image\n",
        "    npimg = img.numpy()  # Convert the tensor to a numpy array\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # Reshape and display the image\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yP7S8PazBoD5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform: converts images to PyTorch tensors and normalizes them\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n"
      ],
      "metadata": {
        "id": "yJAgqh0TBrMa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset loading\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "cQpswSgnBw4E",
        "outputId": "e314aba9-eaa1-4ea5-cb89-bd8218d3c1b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 505kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.63MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.72MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Autoencoder(encoding_dim=64).to(device)\n",
        "criterion = nn.MSELoss()  # Loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Optimizer"
      ],
      "metadata": {
        "id": "W8NemtCtB35p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader:\n",
        "        img, _ = data\n",
        "        img = img.to(device)\n",
        "\n",
        "        with autocast():\n",
        "            output = model(img)\n",
        "            loss = criterion(output, img)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "    print('Epoch [{}/{}], Loss:{:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRk72snY6LPD",
        "outputId": "9c4229c4-7f64-4b18-cc1e-b718aa3f8d08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1361577664.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-1361577664.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss:0.9273\n",
            "Epoch [2/10], Loss:0.9241\n",
            "Epoch [3/10], Loss:0.9282\n",
            "Epoch [4/10], Loss:0.9259\n",
            "Epoch [5/10], Loss:0.9238\n",
            "Epoch [6/10], Loss:0.9263\n",
            "Epoch [7/10], Loss:0.9211\n",
            "Epoch [8/10], Loss:0.9226\n",
            "Epoch [9/10], Loss:0.9278\n",
            "Epoch [10/10], Loss:0.9238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FCsJeFk2B2Fi",
        "outputId": "005dc5ed-0194-445e-909b-a25bddc464cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Representation\n",
            "tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
            "\n",
            "\n",
            "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          ...,\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "          [-1., -1., -1.,  ..., -1., -1., -1.]]]], device='cuda:0')\n",
            "Latent space representations:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 38.4952,  0.0000,\n",
            "         40.6157, 34.9577,  0.0000,  0.0000,  0.0000, 44.8749, 54.4285,  0.0000,\n",
            "          0.0000, 88.2080,  0.0000,  0.0000,  0.0000,  0.0000, 23.9277,  0.0000,\n",
            "          0.0000, 34.0677,  0.0000, 24.8186, 46.5826, 11.2281,  0.0000,  0.0000,\n",
            "          0.0000,  7.1625,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 47.3144, 21.2201,  0.0000,  5.0906, 38.3228,\n",
            "          0.0000,  0.0000,  0.0000, 37.7726,  0.0000, 21.9207,  0.0000, 17.9367,\n",
            "         43.9013,  0.0000,  0.0000,  0.0000,  0.0000, 32.1348,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 42.0871,  0.0000,\n",
            "         43.8111, 37.2712,  0.0000,  0.0000,  0.0000, 48.3840, 58.5826,  0.0000,\n",
            "          0.0000, 95.1928,  0.0000,  0.0000,  0.0000,  0.0000, 25.5764,  0.0000,\n",
            "          0.0000, 36.5512,  0.0000, 26.0098, 50.0244, 12.2856,  0.0000,  0.0000,\n",
            "          0.0000,  8.1348,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 50.4919, 23.2317,  0.0000,  6.0074, 41.1377,\n",
            "          0.0000,  0.0000,  0.0000, 40.6387,  0.0000, 23.4145,  0.0000, 19.8148,\n",
            "         46.6224,  0.0000,  0.0000,  0.0000,  0.0000, 34.4650,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 38.1800,  0.0000,\n",
            "         39.3748, 34.4727,  0.0000,  0.0000,  0.0000, 44.2780, 53.2456,  0.0000,\n",
            "          0.0000, 86.3347,  0.0000,  0.0000,  0.0000,  0.0000, 24.1871,  0.0000,\n",
            "          0.0000, 32.9773,  0.0000, 24.1814, 45.7228, 11.0917,  0.0000,  0.0000,\n",
            "          0.0000,  7.2467,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 46.3399, 20.6823,  0.0000,  5.5900, 37.3876,\n",
            "          0.0000,  0.0000,  0.0000, 36.9507,  0.0000, 21.2825,  0.0000, 18.0585,\n",
            "         42.4769,  0.0000,  0.0000,  0.0000,  0.0000, 31.6035,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 42.3273,  0.0000,\n",
            "         43.9618, 38.2751,  0.0000,  0.0000,  0.0000, 49.6734, 58.9274,  0.0000,\n",
            "          0.0000, 96.1447,  0.0000,  0.0000,  0.0000,  0.0000, 26.6762,  0.0000,\n",
            "          0.0000, 37.0108,  0.0000, 26.9520, 50.9068, 12.7023,  0.0000,  0.0000,\n",
            "          0.0000,  8.4868,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, 51.9339, 23.1537,  0.0000,  6.8171, 41.6906,\n",
            "          0.0000,  0.0000,  0.0000, 41.2029,  0.0000, 23.7728,  0.0000, 19.9567,\n",
            "         47.4531,  0.0000,  0.0000,  0.0000,  0.0000, 35.6270,  0.0000,  0.0000]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HpaXgH_WACZr"
      }
    }
  ]
}